{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-05T16:44:39.245826Z",
     "iopub.status.busy": "2025-04-05T16:44:39.245538Z",
     "iopub.status.idle": "2025-04-05T16:44:51.244507Z",
     "shell.execute_reply": "2025-04-05T16:44:51.243299Z",
     "shell.execute_reply.started": "2025-04-05T16:44:39.245797Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shap in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
      "Collecting grad-cam\n",
      "  Downloading grad-cam-1.5.4.tar.gz (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.26.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.2)\n",
      "Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.10/dist-packages (from shap) (0.0.7)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.60.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (3.1.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from grad-cam) (11.0.0)\n",
      "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from grad-cam) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from grad-cam) (0.20.1+cu121)\n",
      "Collecting ttach (from grad-cam)\n",
      "  Downloading ttach-0.0.3-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from grad-cam) (4.10.0.84)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from grad-cam) (3.7.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.7.1->grad-cam) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->shap) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->shap) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->shap) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->shap) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->shap) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->shap) (2.4.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.43.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2025.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.1->grad-cam) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->shap) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->shap) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->shap) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->shap) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->shap) (2024.2.0)\n",
      "Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
      "Building wheels for collected packages: grad-cam\n",
      "  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for grad-cam: filename=grad_cam-1.5.4-py3-none-any.whl size=39680 sha256=c3a9e6f0e66703e3b2ea43cb561e92dde0fb7455d14dc90d0b98607913e6a499\n",
      "  Stored in directory: /root/.cache/pip/wheels/50/b0/82/1f97b5348c7fe9f0ce0ba18497202cafa5dec4562bd5292680\n",
      "Successfully built grad-cam\n",
      "Installing collected packages: ttach, grad-cam\n",
      "Successfully installed grad-cam-1.5.4 ttach-0.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install shap grad-cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T16:44:51.246021Z",
     "iopub.status.busy": "2025-04-05T16:44:51.245663Z",
     "iopub.status.idle": "2025-04-05T16:44:55.203903Z",
     "shell.execute_reply": "2025-04-05T16:44:55.202758Z",
     "shell.execute_reply.started": "2025-04-05T16:44:51.245986Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-explain\n",
      "  Downloading tf_explain-0.3.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Downloading tf_explain-0.3.1-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tf-explain\n",
      "Successfully installed tf-explain-0.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T16:44:55.205010Z",
     "iopub.status.busy": "2025-04-05T16:44:55.204715Z",
     "iopub.status.idle": "2025-04-05T16:44:58.808894Z",
     "shell.execute_reply": "2025-04-05T16:44:58.807770Z",
     "shell.execute_reply.started": "2025-04-05T16:44:55.204983Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.4.7)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (3.5.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.32.3)\n",
      "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.26.4)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.0.8)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (3.12.1)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.13.1)\n",
      "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2025.1.31)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->keras->keras-tuner) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->keras->keras-tuner) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->keras->keras-tuner) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->keras->keras-tuner) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->keras->keras-tuner) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->keras->keras-tuner) (2.4.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->keras->keras-tuner) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->keras->keras-tuner) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->keras->keras-tuner) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->keras->keras-tuner) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->keras->keras-tuner) (2024.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T10:48:26.241275Z",
     "iopub.status.busy": "2025-06-16T10:48:26.240989Z",
     "iopub.status.idle": "2025-06-16T11:24:16.618661Z",
     "shell.execute_reply": "2025-06-16T11:24:16.617216Z",
     "shell.execute_reply.started": "2025-06-16T10:48:26.241247Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 Complete [00h 01m 37s]\n",
      "val_mae: 0.5935455858707428\n",
      "\n",
      "Best val_mae So Far: 0.46179117262363434\n",
      "Total elapsed time: 00h 35m 09s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adamw', because it has 1 variables whereas the saved optimizer has 105 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 142ms/step - loss: 1.7586 - mae: 0.3869\n",
      "Test Loss (Scaled): 1.8496322631835938, Test MAE (Scaled): 0.4345267117023468\n",
      "Test MAE (Original Scale): 7.485883473334514\n",
      "Overall Accuracy (±0.5 years): 9.52%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'r2_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1a04bbeff4a9>\u001b[0m in \u001b[0;36m<cell line: 336>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;31m# Calculate R-squared in original scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m \u001b[0mr2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_original\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"R² Score (Original Scale): {r2:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'r2_score' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Flatten, Dropout, BatchNormalization, Conv2D, MaxPooling2D, Add, LayerNormalization, Multiply\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy.sparse import csr_matrix\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from keras_tuner.tuners import RandomSearch  # Updated import\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create output directory\n",
    "output_dir = '/kaggle/working/models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load tabular data\n",
    "tabular_data_path = '/kaggle/input/asifazmain/tabulardata1.csv'\n",
    "tabular_data = pd.read_csv(tabular_data_path)\n",
    "\n",
    "# Preprocessing Tabular Data\n",
    "tabular_features = tabular_data.drop(columns=[\"faceImage\"])\n",
    "tabular_labels = tabular_data[\"Age(years)\"]\n",
    "\n",
    "# Normalize age labels\n",
    "label_scaler = StandardScaler()\n",
    "y_tabular_scaled = label_scaler.fit_transform(tabular_labels.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Save label scaler\n",
    "joblib.dump(label_scaler, os.path.join(output_dir, 'label_scaler.pkl'))\n",
    "\n",
    "# Extract features and labels\n",
    "X_tabular = tabular_features.drop(columns=[\"Age(years)\"])\n",
    "y_tabular = y_tabular_scaled\n",
    "\n",
    "# Handle categorical and numerical features\n",
    "categorical_features = [\"Blood Pressure (s/d)\"]\n",
    "numerical_features = [col for col in X_tabular.columns if col not in categorical_features]\n",
    "\n",
    "# Preprocessing pipelines\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "# Combine transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numerical_transformer, numerical_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    sparse_threshold=0\n",
    ")\n",
    "\n",
    "# Preprocess tabular data\n",
    "X_tabular_preprocessed = preprocessor.fit_transform(X_tabular)\n",
    "\n",
    "# Save preprocessor\n",
    "joblib.dump(preprocessor, os.path.join(output_dir, 'preprocessor.pkl'))\n",
    "\n",
    "# Convert sparse to dense if needed\n",
    "if isinstance(X_tabular_preprocessed, csr_matrix):\n",
    "    X_tabular_preprocessed = X_tabular_preprocessed.toarray()\n",
    "\n",
    "# Image IDs\n",
    "image_ids = tabular_data[\"faceImage\"]\n",
    "\n",
    "# Split data\n",
    "X_tabular_train, X_tabular_test, y_train, y_test, image_ids_train, image_ids_test = train_test_split(\n",
    "    X_tabular_preprocessed, y_tabular, image_ids, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Image preprocessing\n",
    "image_data_path = '/kaggle/input/asifazmain/imagedata/imagedata/'\n",
    "image_size = (128, 128)\n",
    "batch_size = 4\n",
    "\n",
    "train_image_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_image_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "train_image_generator = train_image_datagen.flow_from_dataframe(\n",
    "    pd.DataFrame({'filename': image_ids_train.apply(lambda x: f'{x}.jpg')}),\n",
    "    directory=image_data_path,\n",
    "    x_col='filename',\n",
    "    y_col=None,\n",
    "    target_size=image_size,\n",
    "    class_mode=None,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_image_generator = test_image_datagen.flow_from_dataframe(\n",
    "    pd.DataFrame({'filename': image_ids_test.apply(lambda x: f'{x}.jpg')}),\n",
    "    directory=image_data_path,\n",
    "    x_col='filename',\n",
    "    y_col=None,\n",
    "    target_size=image_size,\n",
    "    class_mode=None,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Align images and tabular data\n",
    "def create_tf_dataset(image_gen, tabular_data, labels, batch_size):\n",
    "    images = []\n",
    "    valid_indices = []\n",
    "    for i in range(len(image_gen)):\n",
    "        batch = image_gen[i]\n",
    "        batch_size_actual = batch.shape[0]\n",
    "        start_idx = i * image_gen.batch_size\n",
    "        end_idx = start_idx + batch_size_actual\n",
    "        if end_idx > len(tabular_data):\n",
    "            batch = batch[:len(tabular_data) - start_idx]\n",
    "            images.append(batch)\n",
    "            valid_indices.extend(range(start_idx, start_idx + batch.shape[0]))\n",
    "            break\n",
    "        images.append(batch)\n",
    "        valid_indices.extend(range(start_idx, end_idx))\n",
    "    \n",
    "    images = np.concatenate(images, axis=0)\n",
    "    tabular_data = tabular_data[valid_indices]\n",
    "    labels = labels[valid_indices]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            'image_input': images,\n",
    "            'tabular_input': tabular_data.astype(np.float32)\n",
    "        },\n",
    "        labels.astype(np.float32)\n",
    "    ))\n",
    "    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = create_tf_dataset(train_image_generator, X_tabular_train, y_train, batch_size)\n",
    "test_dataset = create_tf_dataset(test_image_generator, X_tabular_test, y_test, batch_size)\n",
    "\n",
    "# CNN model\n",
    "def build_model(hp):\n",
    "    image_input = Input(shape=(*image_size, 3), name=\"image_input\")\n",
    "    x = Conv2D(\n",
    "        filters=hp.Int('conv1_filters', min_value=32, max_value=96, step=32),\n",
    "        kernel_size=3,\n",
    "        activation=\"relu\",\n",
    "        padding=\"same\"\n",
    "    )(image_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=2)(x)\n",
    "    \n",
    "    shortcut = x\n",
    "    x = Conv2D(\n",
    "        filters=hp.Int('conv2_filters', min_value=64, max_value=128, step=32),\n",
    "        kernel_size=3,\n",
    "        activation=\"relu\",\n",
    "        padding=\"same\"\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    shortcut = Conv2D(\n",
    "        filters=hp.Int('conv2_filters', min_value=64, max_value=128, step=32),\n",
    "        kernel_size=1,\n",
    "        padding=\"same\"\n",
    "    )(shortcut)\n",
    "    x = Add()([x, shortcut])\n",
    "    \n",
    "    x = Conv2D(\n",
    "        filters=hp.Int('conv3_filters', min_value=96, max_value=192, step=32),\n",
    "        kernel_size=3,\n",
    "        activation=\"relu\",\n",
    "        padding=\"same\"\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=2)(x)\n",
    "    \n",
    "    shortcut = x\n",
    "    x = Conv2D(\n",
    "        filters=hp.Int('conv4_filters', min_value=128, max_value=256, step=64),\n",
    "        kernel_size=3,\n",
    "        activation=\"relu\",\n",
    "        padding=\"same\"\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    shortcut = Conv2D(\n",
    "        filters=hp.Int('conv4_filters', min_value=128, max_value=256, step=64),\n",
    "        kernel_size=1,\n",
    "        padding=\"same\"\n",
    "    )(shortcut)\n",
    "    x = Add()([x, shortcut])\n",
    "    \n",
    "    x = Conv2D(\n",
    "        filters=hp.Int('conv5_filters', min_value=192, max_value=384, step=64),\n",
    "        kernel_size=3,\n",
    "        dilation_rate=2,\n",
    "        activation=\"relu\",\n",
    "        padding=\"same\"\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=2)(x)\n",
    "    \n",
    "    x = Conv2D(\n",
    "        filters=hp.Int('conv6_filters', min_value=256, max_value=512, step=64),\n",
    "        kernel_size=3,\n",
    "        activation=\"relu\",\n",
    "        padding=\"same\"\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(\n",
    "        units=hp.Int('image_dense_units', min_value=128, max_value=512, step=128),\n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=l2(hp.Float('l2_reg', min_value=1e-4, max_value=5e-2, sampling='log'))\n",
    "    )(x)\n",
    "    x = Dropout(hp.Float('dropout_image', min_value=0.3, max_value=0.6))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    image_output = Dense(64, activation=\"relu\", name=\"image_output\")(x)\n",
    "\n",
    "    tabular_input = Input(shape=(X_tabular_preprocessed.shape[1],), name=\"tabular_input\")\n",
    "    y = Dense(\n",
    "        units=hp.Int('tabular_units_1', min_value=128, max_value=512, step=128),\n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=l2(hp.Float('l2_reg', min_value=1e-4, max_value=5e-2, sampling='log'))\n",
    "    )(tabular_input)\n",
    "    y = LayerNormalization()(y)\n",
    "    y = Dropout(hp.Float('dropout_tabular_1', min_value=0.3, max_value=0.6))(y)\n",
    "    y = Dense(\n",
    "        units=hp.Int('tabular_units_2', min_value=64, max_value=256, step=64),\n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=l2(hp.Float('l2_reg', min_value=1e-4, max_value=5e-2, sampling='log'))\n",
    "    )(y)\n",
    "    y = LayerNormalization()(y)\n",
    "    tabular_output = Dense(64, activation=\"relu\", name=\"tabular_output\")(y)\n",
    "\n",
    "    concatenated = Concatenate()([image_output, tabular_output])\n",
    "    fused = Multiply()([image_output, tabular_output])\n",
    "    fused = Dense(64, activation=\"relu\")(fused)\n",
    "    combined = Concatenate()([concatenated, fused])\n",
    "    \n",
    "    z = Dense(\n",
    "        units=hp.Int('concat_units', min_value=128, max_value=512, step=128),\n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=l2(hp.Float('l2_reg', min_value=1e-4, max_value=5e-2, sampling='log'))\n",
    "    )(combined)\n",
    "    z = Dropout(hp.Float('dropout_concat', min_value=0.3, max_value=0.6))(z)\n",
    "    z = BatchNormalization()(z)\n",
    "    final_output = Dense(1, activation=\"linear\", name=\"final_output\")(z)\n",
    "\n",
    "    lr_schedule = CosineDecay(\n",
    "        initial_learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-3, sampling='log'),\n",
    "        decay_steps=10000\n",
    "    )\n",
    "    model = Model(inputs=[image_input, tabular_input], outputs=final_output)\n",
    "    model.compile(\n",
    "        optimizer=AdamW(learning_rate=lr_schedule, weight_decay=1e-4, clipnorm=1.0),\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Hyperparameter tuning\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_mae',\n",
    "    max_trials=15,\n",
    "    executions_per_trial=2,\n",
    "    directory='tuner_results',\n",
    "    project_name='cnn_6layers_low_mae'\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_mae', patience=50, restore_best_weights=True, mode='min')\n",
    "checkpoint = ModelCheckpoint(\n",
    "    os.path.join(output_dir, 'best_model.keras'),  # Changed to .keras\n",
    "    monitor='val_mae',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Perform tuning\n",
    "tuner.search(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=100,\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")\n",
    "\n",
    "# Get best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Save the final model explicitly\n",
    "best_model.save(os.path.join(output_dir, 'final_model.keras'))  # Changed to .keras\n",
    "\n",
    "# Evaluate with additional metrics\n",
    "loss, mae_scaled = best_model.evaluate(test_dataset)\n",
    "print(f\"Test Loss (Scaled): {loss}, Test MAE (Scaled): {mae_scaled}\")\n",
    "\n",
    "# Convert MAE back to original scale\n",
    "mae_original = label_scaler.inverse_transform([[mae_scaled]])[0][0] - label_scaler.inverse_transform([[0]])[0][0]\n",
    "print(f\"Test MAE (Original Scale): {mae_original}\")\n",
    "\n",
    "# Get predictions for the test dataset\n",
    "y_pred_scaled = []\n",
    "y_true_scaled = []\n",
    "for batch in test_dataset:\n",
    "    inputs, labels = batch\n",
    "    preds = best_model.predict(inputs, verbose=0)\n",
    "    y_pred_scaled.extend(preds.flatten())\n",
    "    y_true_scaled.extend(labels.numpy().flatten())\n",
    "\n",
    "y_pred_scaled = np.array(y_pred_scaled)\n",
    "y_true_scaled = np.array(y_true_scaled)\n",
    "\n",
    "# Inverse-transform predictions and true labels to original scale\n",
    "y_pred_original = label_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_true_original = label_scaler.inverse_transform(y_true_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate overall accuracy (±0.5 years in original scale)\n",
    "accuracy_0_5 = np.mean(np.abs(y_pred_original - y_true_original) <= 0.5) * 100\n",
    "print(f\"Overall Accuracy (±0.5 years): {accuracy_0_5:.2f}%\")\n",
    "\n",
    "# Calculate R-squared in original scale\n",
    "r2 = r2_score(y_true_original, y_pred_original)\n",
    "print(f\"R² Score (Original Scale): {r2:.4f}\")\n",
    "\n",
    "# Optional: Accuracy with rounded ages\n",
    "accuracy_rounded = np.mean(np.round(y_pred_original) == np.round(y_true_original)) * 100\n",
    "print(f\"Accuracy (Rounded to Nearest Integer): {accuracy_rounded:.2f}%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6665437,
     "sourceId": 10747595,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6780023,
     "sourceId": 10907731,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7056826,
     "sourceId": 11286490,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
