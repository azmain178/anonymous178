{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Concatenate, Flatten, Dropout, BatchNormalization, Conv2D, MaxPooling2D, Add, LayerNormalization, Multiply\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom scipy.sparse import csr_matrix\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.optimizers import AdamW\nfrom tensorflow.keras.optimizers.schedules import CosineDecay\nfrom keras_tuner.tuners import RandomSearch  # Updated import\nfrom tensorflow.keras.regularizers import l2\nimport joblib\nimport os\n\n# Create output directory\noutput_dir = '/kaggle/working/models'\nos.makedirs(output_dir, exist_ok=True)\n\n# Load tabular data\ntabular_data_path = '/kaggle/input/asifazmain/tabulardata1.csv'\ntabular_data = pd.read_csv(tabular_data_path)\n\n# Preprocessing Tabular Data\ntabular_features = tabular_data.drop(columns=[\"faceImage\"])\ntabular_labels = tabular_data[\"Age(years)\"]\n\n# Normalize age labels\nlabel_scaler = StandardScaler()\ny_tabular_scaled = label_scaler.fit_transform(tabular_labels.values.reshape(-1, 1)).flatten()\n\n# Save label scaler\njoblib.dump(label_scaler, os.path.join(output_dir, 'label_scaler.pkl'))\n\n# Extract features and labels\nX_tabular = tabular_features.drop(columns=[\"Age(years)\"])\ny_tabular = y_tabular_scaled\n\n# Handle categorical and numerical features\ncategorical_features = [\"Blood Pressure (s/d)\"]\nnumerical_features = [col for col in X_tabular.columns if col not in categorical_features]\n\n# Preprocessing pipelines\nnumerical_transformer = StandardScaler()\ncategorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n\n# Combine transformers\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numerical_transformer, numerical_features),\n        (\"cat\", categorical_transformer, categorical_features)\n    ],\n    sparse_threshold=0\n)\n\n# Preprocess tabular data\nX_tabular_preprocessed = preprocessor.fit_transform(X_tabular)\n\n# Save preprocessor\njoblib.dump(preprocessor, os.path.join(output_dir, 'preprocessor.pkl'))\n\n# Convert sparse to dense if needed\nif isinstance(X_tabular_preprocessed, csr_matrix):\n    X_tabular_preprocessed = X_tabular_preprocessed.toarray()\n\n# Image IDs\nimage_ids = tabular_data[\"faceImage\"]\n\n# Split data\nX_tabular_train, X_tabular_test, y_train, y_test, image_ids_train, image_ids_test = train_test_split(\n    X_tabular_preprocessed, y_tabular, image_ids, test_size=0.2, random_state=42\n)\n\n# Image preprocessing\nimage_data_path = '/kaggle/input/asifazmain/imagedata/imagedata/'\nimage_size = (128, 128)\nbatch_size = 4\n\ntrain_image_datagen = ImageDataGenerator(\n    rescale=1.0/255.0,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    brightness_range=[0.8, 1.2],\n    fill_mode='nearest'\n)\n\ntest_image_datagen = ImageDataGenerator(rescale=1.0/255.0)\n\ntrain_image_generator = train_image_datagen.flow_from_dataframe(\n    pd.DataFrame({'filename': image_ids_train.apply(lambda x: f'{x}.jpg')}),\n    directory=image_data_path,\n    x_col='filename',\n    y_col=None,\n    target_size=image_size,\n    class_mode=None,\n    batch_size=batch_size,\n    shuffle=False\n)\n\ntest_image_generator = test_image_datagen.flow_from_dataframe(\n    pd.DataFrame({'filename': image_ids_test.apply(lambda x: f'{x}.jpg')}),\n    directory=image_data_path,\n    x_col='filename',\n    y_col=None,\n    target_size=image_size,\n    class_mode=None,\n    batch_size=batch_size,\n    shuffle=False\n)\n\n# Align images and tabular data\ndef create_tf_dataset(image_gen, tabular_data, labels, batch_size):\n    images = []\n    valid_indices = []\n    for i in range(len(image_gen)):\n        batch = image_gen[i]\n        batch_size_actual = batch.shape[0]\n        start_idx = i * image_gen.batch_size\n        end_idx = start_idx + batch_size_actual\n        if end_idx > len(tabular_data):\n            batch = batch[:len(tabular_data) - start_idx]\n            images.append(batch)\n            valid_indices.extend(range(start_idx, start_idx + batch.shape[0]))\n            break\n        images.append(batch)\n        valid_indices.extend(range(start_idx, end_idx))\n    \n    images = np.concatenate(images, axis=0)\n    tabular_data = tabular_data[valid_indices]\n    labels = labels[valid_indices]\n    \n    dataset = tf.data.Dataset.from_tensor_slices((\n        {\n            'image_input': images,\n            'tabular_input': tabular_data.astype(np.float32)\n        },\n        labels.astype(np.float32)\n    ))\n    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n# Create datasets\ntrain_dataset = create_tf_dataset(train_image_generator, X_tabular_train, y_train, batch_size)\ntest_dataset = create_tf_dataset(test_image_generator, X_tabular_test, y_test, batch_size)\n\n# CNN model\ndef build_model(hp):\n    image_input = Input(shape=(*image_size, 3), name=\"image_input\")\n    x = Conv2D(\n        filters=hp.Int('conv1_filters', min_value=32, max_value=96, step=32),\n        kernel_size=3,\n        activation=\"relu\",\n        padding=\"same\"\n    )(image_input)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D(pool_size=2)(x)\n    \n    shortcut = x\n    x = Conv2D(\n        filters=hp.Int('conv2_filters', min_value=64, max_value=128, step=32),\n        kernel_size=3,\n        activation=\"relu\",\n        padding=\"same\"\n    )(x)\n    x = BatchNormalization()(x)\n    shortcut = Conv2D(\n        filters=hp.Int('conv2_filters', min_value=64, max_value=128, step=32),\n        kernel_size=1,\n        padding=\"same\"\n    )(shortcut)\n    x = Add()([x, shortcut])\n    \n    x = Conv2D(\n        filters=hp.Int('conv3_filters', min_value=96, max_value=192, step=32),\n        kernel_size=3,\n        activation=\"relu\",\n        padding=\"same\"\n    )(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D(pool_size=2)(x)\n    \n    shortcut = x\n    x = Conv2D(\n        filters=hp.Int('conv4_filters', min_value=128, max_value=256, step=64),\n        kernel_size=3,\n        activation=\"relu\",\n        padding=\"same\"\n    )(x)\n    x = BatchNormalization()(x)\n    shortcut = Conv2D(\n        filters=hp.Int('conv4_filters', min_value=128, max_value=256, step=64),\n        kernel_size=1,\n        padding=\"same\"\n    )(shortcut)\n    x = Add()([x, shortcut])\n    \n    x = Conv2D(\n        filters=hp.Int('conv5_filters', min_value=192, max_value=384, step=64),\n        kernel_size=3,\n        dilation_rate=2,\n        activation=\"relu\",\n        padding=\"same\"\n    )(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D(pool_size=2)(x)\n    \n    x = Conv2D(\n        filters=hp.Int('conv6_filters', min_value=256, max_value=512, step=64),\n        kernel_size=3,\n        activation=\"relu\",\n        padding=\"same\"\n    )(x)\n    x = BatchNormalization()(x)\n    \n    x = Flatten()(x)\n    x = Dense(\n        units=hp.Int('image_dense_units', min_value=128, max_value=512, step=128),\n        activation=\"relu\",\n        kernel_regularizer=l2(hp.Float('l2_reg', min_value=1e-4, max_value=5e-2, sampling='log'))\n    )(x)\n    x = Dropout(hp.Float('dropout_image', min_value=0.3, max_value=0.6))(x)\n    x = BatchNormalization()(x)\n    image_output = Dense(64, activation=\"relu\", name=\"image_output\")(x)\n\n    tabular_input = Input(shape=(X_tabular_preprocessed.shape[1],), name=\"tabular_input\")\n    y = Dense(\n        units=hp.Int('tabular_units_1', min_value=128, max_value=512, step=128),\n        activation=\"relu\",\n        kernel_regularizer=l2(hp.Float('l2_reg', min_value=1e-4, max_value=5e-2, sampling='log'))\n    )(tabular_input)\n    y = LayerNormalization()(y)\n    y = Dropout(hp.Float('dropout_tabular_1', min_value=0.3, max_value=0.6))(y)\n    y = Dense(\n        units=hp.Int('tabular_units_2', min_value=64, max_value=256, step=64),\n        activation=\"relu\",\n        kernel_regularizer=l2(hp.Float('l2_reg', min_value=1e-4, max_value=5e-2, sampling='log'))\n    )(y)\n    y = LayerNormalization()(y)\n    tabular_output = Dense(64, activation=\"relu\", name=\"tabular_output\")(y)\n\n    concatenated = Concatenate()([image_output, tabular_output])\n    fused = Multiply()([image_output, tabular_output])\n    fused = Dense(64, activation=\"relu\")(fused)\n    combined = Concatenate()([concatenated, fused])\n    \n    z = Dense(\n        units=hp.Int('concat_units', min_value=128, max_value=512, step=128),\n        activation=\"relu\",\n        kernel_regularizer=l2(hp.Float('l2_reg', min_value=1e-4, max_value=5e-2, sampling='log'))\n    )(combined)\n    z = Dropout(hp.Float('dropout_concat', min_value=0.3, max_value=0.6))(z)\n    z = BatchNormalization()(z)\n    final_output = Dense(1, activation=\"linear\", name=\"final_output\")(z)\n\n    lr_schedule = CosineDecay(\n        initial_learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-3, sampling='log'),\n        decay_steps=10000\n    )\n    model = Model(inputs=[image_input, tabular_input], outputs=final_output)\n    model.compile(\n        optimizer=AdamW(learning_rate=lr_schedule, weight_decay=1e-4, clipnorm=1.0),\n        loss=\"mse\",\n        metrics=[\"mae\"]\n    )\n    return model\n\n# Hyperparameter tuning\ntuner = RandomSearch(\n    build_model,\n    objective='val_mae',\n    max_trials=15,\n    executions_per_trial=2,\n    directory='tuner_results',\n    project_name='cnn_6layers_low_mae'\n)\n\n# Callbacks\nearly_stopping = EarlyStopping(monitor='val_mae', patience=50, restore_best_weights=True, mode='min')\ncheckpoint = ModelCheckpoint(\n    os.path.join(output_dir, 'best_model.keras'),  # Changed to .keras\n    monitor='val_mae',\n    save_best_only=True,\n    mode='min',\n    verbose=1\n)\n\n# Perform tuning\ntuner.search(\n    train_dataset,\n    validation_data=test_dataset,\n    epochs=100,\n    callbacks=[early_stopping, checkpoint]\n)\n\n# Get best model\nbest_model = tuner.get_best_models(num_models=1)[0]\n\n# Save the final model explicitly\nbest_model.save(os.path.join(output_dir, 'final_model.keras'))  # Changed to .keras\n\n# Evaluate with additional metrics\nloss, mae_scaled = best_model.evaluate(test_dataset)\nprint(f\"Test Loss (Scaled): {loss}, Test MAE (Scaled): {mae_scaled}\")\n\n# Convert MAE back to original scale\nmae_original = label_scaler.inverse_transform([[mae_scaled]])[0][0] - label_scaler.inverse_transform([[0]])[0][0]\nprint(f\"Test MAE (Original Scale): {mae_original}\")\n\n# Get predictions for the test dataset\ny_pred_scaled = []\ny_true_scaled = []\nfor batch in test_dataset:\n    inputs, labels = batch\n    preds = best_model.predict(inputs, verbose=0)\n    y_pred_scaled.extend(preds.flatten())\n    y_true_scaled.extend(labels.numpy().flatten())\n\ny_pred_scaled = np.array(y_pred_scaled)\ny_true_scaled = np.array(y_true_scaled)\n\n# Inverse-transform predictions and true labels to original scale\ny_pred_original = label_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\ny_true_original = label_scaler.inverse_transform(y_true_scaled.reshape(-1, 1)).flatten()\n\n# Calculate overall accuracy (±0.5 years in original scale)\naccuracy_0_5 = np.mean(np.abs(y_pred_original - y_true_original) <= 0.5) * 100\nprint(f\"Overall Accuracy (±0.5 years): {accuracy_0_5:.2f}%\")\n\n# Calculate R-squared in original scale\nr2 = r2_score(y_true_original, y_pred_original)\nprint(f\"R² Score (Original Scale): {r2:.4f}\")\n\n# Optional: Accuracy with rounded ages\naccuracy_rounded = np.mean(np.round(y_pred_original) == np.round(y_true_original)) * 100\nprint(f\"Accuracy (Rounded to Nearest Integer): {accuracy_rounded:.2f}%\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T12:41:15.930685Z","iopub.execute_input":"2024-09-16T12:41:15.931015Z","iopub.status.idle":"2024-09-16T12:41:15.948321Z","shell.execute_reply.started":"2024-09-16T12:41:15.930976Z","shell.execute_reply":"2024-09-16T12:41:15.947252Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Final training\nhistory = best_model.fit(\n    train_dataset,\n    validation_data=test_dataset,\n    epochs=100,\n    callbacks=[early_stopping, checkpoint]\n)\n\n# Add the corrected evaluation code here\nimport numpy as np\nfrom sklearn.metrics import r2_score  # Added import for r2_score\nimport matplotlib.pyplot as plt\n\n# Get predictions for the test dataset\ny_pred_scaled = []\ny_true_scaled = []\nfor batch in test_dataset:\n    inputs, labels = batch\n    preds = best_model.predict(inputs, verbose=0)\n    y_pred_scaled.extend(preds.flatten())\n    y_true_scaled.extend(labels.numpy().flatten())\n\ny_pred_scaled = np.array(y_pred_scaled)\ny_true_scaled = np.array(y_true_scaled)\n\n# Inverse-transform predictions and true labels to original scale\ny_pred_original = label_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\ny_true_original = label_scaler.inverse_transform(y_true_scaled.reshape(-1, 1)).flatten()\n\n# Calculate overall accuracy (±0.5 years in original scale)\naccuracy_0_5 = np.mean(np.abs(y_pred_original - y_true_original) <= 0.5) * 100\nprint(f\"Overall Accuracy (±0.5 years): {accuracy_0_5:.2f}%\")\n\n# Calculate R-squared in original scale\nr2 = r2_score(y_true_original, y_pred_original)\nprint(f\"R² Score (Original Scale): {r2:.4f}\")\n\n# Optional: Accuracy with rounded ages (exact integer matches)\naccuracy_rounded = np.mean(np.round(y_pred_original) == np.round(y_true_original)) * 100\nprint(f\"Accuracy (Rounded to Nearest Integer): {accuracy_rounded:.2f}%\")\n\n# Plot predicted vs. actual ages\nplt.figure(figsize=(8, 8))\nplt.scatter(y_true_original, y_pred_original, alpha=0.5, label='Predictions')\nplt.plot([y_true_original.min(), y_true_original.max()], [y_true_original.min(), y_true_original.max()], 'r--', label='Ideal')\nplt.xlabel('Actual Age (Years)')\nplt.ylabel('Predicted Age (Years)')\nplt.title('Predicted vs. Actual Ages')\nplt.legend()\nplt.savefig('/kaggle/working/models/pred_vs_actual.png')\nplt.close()\nprint(\"Predicted vs. actual ages plot saved at /kaggle/working/models/pred_vs_actual.png\")\n\n# Optional: Save model in TensorFlow.js format\ntry:\n    import tensorflowjs as tfjs\n    tfjs.converters.save_keras_model(best_model, os.path.join(output_dir, 'tfjs_model'))\n    print(\"TensorFlow.js model saved successfully.\")\nexcept ImportError:\n    print(\"TensorFlow.js not installed. Skipping TF.js model saving.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prediction function (corrected)\ndef predict_age(image_path, height_cm, weight_kg, bmi, blood_pressure, blood_oxygen, blood_sugar, model, preprocessor, label_scaler, image_size=(128, 128)):\n    \"\"\"\n    Predict age using an image and biomarkers.\n    \n    Parameters:\n    - image_path (str): Path to the user's face image\n    - height_cm (float): Height in centimeters\n    - weight_kg (float): Weight in kilograms\n    - bmi (float): Body Mass Index\n    - blood_pressure (str): Blood pressure in 'systolic/diastolic' format (e.g., '120/80')\n    - blood_oxygen (float): Blood oxygen level in percentage\n    - blood_sugar (float): Blood sugar level in mg/dL\n    - model: Trained Keras model (best_model)\n    - preprocessor: Fitted ColumnTransformer from training\n    - label_scaler: Fitted StandardScaler for age labels\n    - image_size (tuple): Target image size (default: (128, 128))\n    \n    Returns:\n    - predicted_age (float): Predicted age in years\n    \"\"\"\n    \n    # 1. Preprocess the image\n    img = load_img(image_path, target_size=image_size)\n    img_array = img_to_array(img) / 255.0  # Rescale as in test_image_datagen\n    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension: (1, 128, 128, 3)\n\n    # 2. Preprocess the tabular data (biomarkers)\n    biomarkers = {\n        'Height (cm)': height_cm,\n        'Weight (kg)': weight_kg,\n        'BMI': bmi,\n        'Blood Pressure (s/d)': blood_pressure,\n        'Blood Oxygen': blood_oxygen,\n        'Blood Sugar(mg/dl)': blood_sugar\n    }\n    \n    # Create DataFrame with all required columns (match training data structure)\n    tabular_df = pd.DataFrame([biomarkers])\n    \n    # Ensure all columns from training are present, fill missing with median or mode from training data\n    for col in X_tabular.columns:\n        if col not in tabular_df.columns:\n            if col in numerical_features:\n                tabular_df[col] = X_tabular[col].median()  # Use median for numerical\n            elif col in categorical_features:\n                tabular_df[col] = X_tabular[col].mode()[0]  # Use mode for categorical\n    \n    # Reorder columns to match training data\n    tabular_df = tabular_df[X_tabular.columns]\n    \n    # Apply preprocessing\n    tabular_processed = preprocessor.transform(tabular_df)\n    if isinstance(tabular_processed, csr_matrix):\n        tabular_processed = tabular_processed.toarray()\n    \n    # Ensure tabular_processed is 2D (batch_size, features)\n    if len(tabular_processed.shape) > 2:\n        tabular_processed = tabular_processed.squeeze()  # Remove extra dimensions if any\n    if len(tabular_processed.shape) == 1:\n        tabular_processed = np.expand_dims(tabular_processed, axis=0)  # Add batch dimension if missing\n    # Now tabular_processed should be (1, 77) or similar, matching the model's expected shape\n\n    # 3. Make prediction\n    inputs = {\n        'image_input': img_array,\n        'tabular_input': tabular_processed.astype(np.float32)\n    }\n    prediction_scaled = model.predict(inputs)[0][0]  # Get single value\n    \n    # 4. Convert to original scale\n    predicted_age = label_scaler.inverse_transform([[prediction_scaled]])[0][0]\n    \n    return predicted_age","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}